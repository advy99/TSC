{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CLASIFICACIÓN DEL RIESGO DE ABANDONO DE LOS CLIENTES DE UN BANCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos con el que vamos a trabajar ahora contiene información sobre los usuarios de un banco. Queremos predecir si los clientes van a dejar de usar los servicios de dicho banco o no. El conjunto de datos consta de 10000 observaciones y 14 variables.\n",
    "\n",
    "La siguiente figura indica cómo cargar el conjunto de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una matriz con las variables de entrada y otra matriz con la variable de salida (objetivo, columna 14). Excluiremos la columna 1 y 2 que son ‘row_number’ y ‘customerid’ ya que no nos aportan información útil para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,3:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 'France', 'Female', 39, 1, 0.0, 2, 0, 0, 93826.63]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer el análisis más sencillo si codificamos las variables no numéricas. Country contiene los valores: ’France, Spain, Germany’ y Gender: ‘Male, Female’. La manera de codificarlo será convertir estas palabras a valores numéricos. Para esto usaremos la función LabelEncoder, de la librería ‘ScikitLearn’, que al darle una cadena de texto nos devuelve valores entre 0 y n_clases-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [608, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [502, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [772, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que Country ahora toma valores del 0 al 2 mientras que male y female fueron reemplazados por 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función train_test_split de la librería ScikitLearn para dividir nuestros datos.\n",
    "\n",
    "Usaremos 80% para entrenar el modelo y 20% para validarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[709, 1, 1, ..., 1, 0, 145251.35],\n",
       "        [786, 2, 0, ..., 1, 0, 117034.32],\n",
       "        [648, 1, 0, ..., 1, 0, 190994.48],\n",
       "        ...,\n",
       "        [666, 0, 0, ..., 1, 0, 50908.48],\n",
       "        [695, 0, 0, ..., 1, 1, 141756.32],\n",
       "        [595, 1, 1, ..., 1, 0, 48421.4]], dtype=object),\n",
       " array([[684, 0, 1, ..., 1, 1, 71725.73],\n",
       "        [447, 0, 0, ..., 1, 1, 151815.76],\n",
       "        [485, 1, 1, ..., 1, 0, 51113.14],\n",
       "        ...,\n",
       "        [660, 0, 1, ..., 1, 0, 13218.6],\n",
       "        [550, 0, 1, ..., 1, 1, 133501.94],\n",
       "        [585, 0, 1, ..., 1, 1, 55346.14]], dtype=object),\n",
       " array([0, 0, 1, ..., 0, 0, 1]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observamos los datos detenidamente podemos apreciar que hay variables cuyos valores pueden\n",
    "ser muy variados, desde muy altos a muy pequeños por esta razón escalaremos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.60628195,  0.29962659,  0.91094795, ...,  0.64959174,\n",
       "         -1.03797037,  0.78365451],\n",
       "        [ 1.41054777,  1.50234244, -1.09775756, ...,  0.64959174,\n",
       "         -1.03797037,  0.29178878],\n",
       "        [-0.0308637 ,  0.29962659, -1.09775756, ...,  0.64959174,\n",
       "         -1.03797037,  1.58102679],\n",
       "        ...,\n",
       "        [ 0.15714649, -0.90308927, -1.09775756, ...,  0.64959174,\n",
       "         -1.03797037, -0.86088511],\n",
       "        [ 0.4600518 , -0.90308927, -1.09775756, ...,  0.64959174,\n",
       "          0.96341863,  0.72273082],\n",
       "        [-0.58444927,  0.29962659,  0.91094795, ...,  0.64959174,\n",
       "         -1.03797037, -0.9042387 ]]),\n",
       " array([[ 0.34515668, -0.90308927,  0.91094795, ...,  0.64959174,\n",
       "          0.96341863, -0.49800881],\n",
       "        [-2.13031084, -0.90308927, -1.09775756, ...,  0.64959174,\n",
       "          0.96341863,  0.89808215],\n",
       "        [-1.73340044,  0.29962659,  0.91094795, ...,  0.64959174,\n",
       "         -1.03797037, -0.85731758],\n",
       "        ...,\n",
       "        [ 0.09447643, -0.90308927,  0.91094795, ...,  0.64959174,\n",
       "         -1.03797037, -1.51787701],\n",
       "        [-1.05447474, -0.90308927,  0.91094795, ...,  0.64959174,\n",
       "          0.96341863,  0.57884443],\n",
       "        [-0.68889937, -0.90308927,  0.91094795, ...,  0.64959174,\n",
       "          0.96341863, -0.78352996]]),\n",
       " array([0, 0, 1, ..., 0, 0, 1]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez escalados los datos, pasamos a construir la red neuronal. Importamos Keras, usamos el módulo Sequential para inicializar la red y el modelo Dense para añadir capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la red con Sequential()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las capas usando la función Dense. Indicamos el número de nodos que queremos añadir con output_dim, Init es la inicialización del descenso de gradiente estocástico. Los pesos iniciales serán una variable aleatoria uniforme. Input_dim sólo es necesaria en la primera capa para que el modelo sepa la cantidad de variables que va a recibir, en nuestro caso 11. A partir de aquí las siguientes capas heredarán esta cualidad de la primera capa. La función de activación que utilizaremos será relu en las dos primeras capas (cuanto más cerca tenga su valor a 1, la neurona estará más activada y tendrá más interacción) y en la capa final hemos utilizado la función sigmoide ya que nuestro objetivo es clasificar.\n",
    "\n",
    "Una vez que tenemos la configuración específica de la red, la siguiente tarea es compilarla, para eso utilizamos la función Compile. El primer argumento de esta función es Optimizer que indica el método para entrenar los pesos. Adam es un algoritmo que se basa en el cálculo del descenso del Gradiente Estocástico. El segundo parámetro es loss, este usará la función ‘binary_crossentropy’ para clasificar en 2 categorías. Si tuviéramos más categorías utilizaríamos la función ‘categorical_crossentropy’. Para saber la bondad de nuestra red neuronal utilizaremos la métrica accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation = 'relu', input_shape = (10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función fit para ajustar los pesos de la red. Batch_size para especificar el número de observaciones que necesita entrenar antes de actualizar los pesos. Epoch nos indica el número de iteraciones que realizaremos en el entrenamiento. La estimación de estos parámetros se tiene que hacer por ensayo-error, probando con diferentes valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.4474 - accuracy: 0.8031\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4118 - accuracy: 0.8263\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3911 - accuracy: 0.8369\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.3674 - accuracy: 0.8499\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3579 - accuracy: 0.8555\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.3543 - accuracy: 0.8579\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3523 - accuracy: 0.8575\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3505 - accuracy: 0.8580\n",
      "Epoch 9/100\n",
      "4209/8000 [==============>...............] - ETA: 12s - loss: 0.3425 - accuracy: 0.8622"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la predicción sobre nuestro conjunto de test lo haremos mediante la siguiente expresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción nos proporcionará la probabilidad de pertenecer a un grupo u otro, de tal manera que aquellos valores mayores que 0.5 serán 1 y el resto 0.\n",
    "\n",
    "Creamos una matriz de confusión y vemos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1533,   54],\n",
       "       [ 206,  207]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1587\n",
      "           1       0.79      0.50      0.61       413\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      2000\n",
      "   macro avg       0.84      0.73      0.77      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
